import os
import pandas as pd
from dotenv import load_dotenv
from azure.core.credentials import AzureKeyCredential
from azure.search.documents import SearchClient
from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient
from azure.search.documents.indexes.models import (
    SearchIndex,
    SearchIndexer,
    SearchIndexerIndexProjection,
    SearchIndexerIndexProjectionSelector,
    SearchIndexerIndexProjectionsParameters,
    IndexingParameters,
    IndexingParametersConfiguration,
    SoftDeleteColumnDeletionDetectionPolicy,
    FieldMapping,
    SimpleField,
    SearchableField,
    SearchFieldDataType,
    SearchField,
    VectorSearch,
    HnswAlgorithmConfiguration,
    HnswParameters,
    VectorSearchProfile,
    AzureOpenAIVectorizer,
    AzureOpenAIVectorizerParameters,
    AzureOpenAIEmbeddingSkill,
    SplitSkill,
    IndexProjectionMode,
    InputFieldMappingEntry,
    OutputFieldMappingEntry,
    SemanticSearch,
    SemanticConfiguration,
    SemanticField,
    SemanticPrioritizedFields,
    SearchIndexerDataSourceConnection,
    SearchIndexerDataContainer,
    SearchIndexerSkillset,
    SearchIndexerIndexProjectionSelector,
    SearchIndexerDataSourceType as DataSourceType
)
from azure.storage.blob import BlobServiceClient
from openai import AzureOpenAI
import requests

# Load environment variables from .env file
load_dotenv()

# Azure Blob Storage settings
BLOB_CONNECTION_STRING = os.getenv("AZURE_BLOB_CONNECTION_STRING")
BLOB_CONTAINER_NAME = os.getenv("AZURE_BLOB_CONTAINER_NAME")
BLOB_DATA_SOURCE_NAME = os.getenv("AZURE_BLOB_DATA_SOURCE_NAME")

# Azure AI Search (Vector Store) settings
SEARCH_ENDPOINT = os.getenv("AZURE_AI_SEARCH_ENDPOINT")
SEARCH_API_KEY = os.getenv("AZURE_AI_SEARCH_API_KEY")
SEARCH_INDEX_NAME =  os.getenv("AZURE_AI_SEARCH_INDEX")
SEARCH_INDEXER_NAME =  os.getenv("AZURE_AI_SEARCH_INDEXER")
SEARCH_SKILLSET_NAME =  os.getenv("AZURE_AI_SEARCH_SKILLSET")

# Azure OpenAI (Embedding) client settings
OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
OPENAI_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")

# Initialize Azure OpenAI client
embedding_client = AzureOpenAI(
    api_version="2024-12-01-preview",
    api_key=OPENAI_API_KEY,
    azure_endpoint=OPENAI_ENDPOINT
)

# Initialize Azure Search Client
search_client = SearchClient(
        endpoint=SEARCH_ENDPOINT,
        index_name=SEARCH_INDEX_NAME,
        credential=AzureKeyCredential(SEARCH_API_KEY)
    )

# Initialize Azure Search Index Client
index_client = SearchIndexClient(
    endpoint=SEARCH_ENDPOINT,
    credential=AzureKeyCredential(SEARCH_API_KEY)
    )

# Initialize Azure Search Indexer Client
indexer_client = SearchIndexerClient(
    endpoint=SEARCH_ENDPOINT,
    credential=AzureKeyCredential(SEARCH_API_KEY)
    )

vectorizer_client = AzureOpenAIVectorizer(
    vectorizer_name="default-vectorizer",
    kind="azureOpenAI",
    parameters=AzureOpenAIVectorizerParameters(
        resource_url=OPENAI_ENDPOINT,
        deployment_name="text-embedding-3-small",
        api_key=OPENAI_API_KEY,
        model_name="text-embedding-3-small"
    )
)

# Create the data source connection for Azure Blob Storage
data_source_connection = SearchIndexerDataSourceConnection(
    name=BLOB_DATA_SOURCE_NAME,
    type=DataSourceType.AZURE_BLOB,
    connection_string=BLOB_CONNECTION_STRING,
    container=SearchIndexerDataContainer(name=BLOB_CONTAINER_NAME),
    data_deletion_detection_policy=SoftDeleteColumnDeletionDetectionPolicy(
        soft_delete_column_name="IsDeleted",
        soft_delete_marker_value="True"
    )
)

def deploy_json_config(filepath, url, api_key):
    headers = {
    "Content-Type": "application/json",
    "api-key": api_key
    }
    with open(filepath) as f:
        index_json = f.read()

    try:
        response = requests.put(url, headers=headers, data=index_json)
        print(f"Json configuration from '{filepath}' has deployed to Azure search (status code: {response.status_code}) ")
    except requests.exceptions.HTTPError as e:
        print(f"Json configuration from '{filepath}' has failed to deploy to Azure search (status code: {response.status_code}) ")

def create_or_update_search_index(index_name):
    # Define the index schema (fields and vector search profile)
    chunk_id_field = SearchField(name="chunk_id", type="Edm.String", key=True, filterable=False, facetable=False, analyzer_name="keyword")
    parent_id_field = SearchField(name="parent_id", type="Edm.String", sortable=False, facetable=False, searchable=False)
    chunk_field = SearchField(name="chunk", type="Edm.String", filterable=False, sortable=False, facetable=False)
    title_field = SearchField(name="title", type="Edm.String", filterable=False, sortable=False, facetable=False)
    text_vector_field = SearchField(
        name="text_vector",
        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),
        searchable=True,
        vector_search_dimensions=1536,
        vector_search_profile_name="default"
        )

    # Define the fields for the index
    fields = [chunk_id_field, parent_id_field, chunk_field, title_field, text_vector_field]

    # Define the vector search configuration
    vector_search = VectorSearch(
        profiles=[
            VectorSearchProfile(
                name="default",
                vectorizer_name="default-vectorizer",
                algorithm_configuration_name="default-hnsw"
            )
        ],
        algorithms=[
            HnswAlgorithmConfiguration(
                name="default-hnsw",
                parameters=HnswParameters(metric="cosine")
            )
        ],
        vectorizers=[vectorizer_client]
    )

    # Define the semantic search configuration
    semantic_config = SemanticConfiguration(
        name="default-semantic-config",
        prioritized_fields=SemanticPrioritizedFields(
            title_field=SemanticField(field_name="title"),
            content_fields=[SemanticField(field_name="chunk")]
        ),
    )

    semantic_search = SemanticSearch(
        default_configuration_name="default-semantic-config",
        configurations=[semantic_config]
    )

    # Create the search index
    index = SearchIndex(
        name=index_name,
        fields=fields,
        vector_search=vector_search,
        semantic_search=semantic_search,
    )

    # Create or update the index
    result = index_client.create_or_update_index(index)
    deploy_json_config("NETwork/index_config.json",f"{SEARCH_ENDPOINT}indexes/{SEARCH_INDEX_NAME}?api-version=2025-05-01-Preview", SEARCH_API_KEY)
    print(f"Index '{index_name}' created or updated.")
    return result

def create_or_update_search_indexer(data_source_name, index_name, indexer_name, skillset_name):
    # Set up indexing parameters for CSV (delimitedText)
    indexer_parameters = IndexingParameters(
        configuration=IndexingParametersConfiguration(
            parsing_mode="default",
            query_timeout=None,
            data_to_extract="contentAndMetadata"
        )
    )

    # Create the indexer
    indexer = SearchIndexer(
        name=indexer_name,
        description="Indexer to index documents and generate embeddings",
        data_source_name=data_source_name,
        target_index_name=index_name,
        parameters=indexer_parameters,
        skillset_name=skillset_name,
        field_mappings=[
            FieldMapping(source_field_name="metadata_storage_name", target_field_name="title"),
            FieldMapping(source_field_name="metadata_storage_path", target_field_name="parent_id"),
            FieldMapping(source_field_name="content", target_field_name="chunk")
        ]
    )

    result = indexer_client.create_or_update_indexer(indexer)
    deploy_json_config("NETwork/indexer_config.json",f"{SEARCH_ENDPOINT}indexers/{SEARCH_INDEXER_NAME}?api-version=2025-05-01-Preview", SEARCH_API_KEY)
    print(f"Indexer '{indexer_name}' created or updated.")
    return result

def create_or_update_indexer_skillset(skillset_name):
    # Create skillset sets
    text_split_skill = SplitSkill(
        name="#1",
        description="Skill to split and chunk documents",
        context="/document",
        default_language_code="en",
        text_split_mode="pages",
        maximum_page_length=2000,
        page_overlap_length=500,
        maximum_pages_to_take=0,
        inputs=[InputFieldMappingEntry(name='text', source="/document/content")],
        outputs=[OutputFieldMappingEntry(name='textItems', target_name="pages")]
    )

    embedding_skill = AzureOpenAIEmbeddingSkill(
        name="#2",
        description="Skill to embed documents",
        context="/document/pages/*",
        deployment_name="text-embedding-3-small",
        model_name="text-embedding-3-small",
        dimensions=1536,
        api_key=OPENAI_API_KEY,
        resource_url=OPENAI_ENDPOINT,
        inputs=[InputFieldMappingEntry(name='text', source="/document/pages/*")],
        outputs=[OutputFieldMappingEntry(name='embedding', target_name="text_vector")]
    )

    skills=[text_split_skill, embedding_skill]

    skillset=SearchIndexerSkillset(
        name=skillset_name,
        description="Skillset to chunk documents and generate embeddings",
        skills=skills,
        index_projection=SearchIndexerIndexProjection(
            selectors=[
                SearchIndexerIndexProjectionSelector(
                    target_index_name=SEARCH_INDEX_NAME,
                    parent_key_field_name="parent_id",
                    source_context="/document/pages/*",
                    mappings=[
                        InputFieldMappingEntry(name="text_vector",source="/document/pages/*/text_vector"),
                        InputFieldMappingEntry(name="chunk",source="/document/pages/*"),
                        InputFieldMappingEntry(name="title",source="/document/title"),
                    ],
                )
            ],
            parameters=SearchIndexerIndexProjectionsParameters(
                projection_mode=IndexProjectionMode.SKIP_INDEXING_PARENT_DOCUMENTS
            )
        )
    )

    indexer_client.create_or_update_skillset(skillset)
    print(f"'{SEARCH_SKILLSET_NAME}' skillset created or updated")

if __name__ == "__main__":
    # Create or update the search indexer data source connection
    indexer_client.create_or_update_data_source_connection(data_source_connection)
    print(f"'{BLOB_DATA_SOURCE_NAME}' data source connection created or updated.")

    # Create or update the search index
    create_or_update_search_index(SEARCH_INDEX_NAME)

    # Create or update the search indexer
    create_or_update_search_indexer(BLOB_DATA_SOURCE_NAME, SEARCH_INDEX_NAME, SEARCH_INDEXER_NAME, SEARCH_SKILLSET_NAME)

    # Create or update the search indexer skill set
    create_or_update_indexer_skillset(SEARCH_SKILLSET_NAME)

    # Run the indexer to load data from blob into the index
    # indexer_client.reset_indexer()
    indexer_client.run_indexer(SEARCH_INDEXER_NAME)
    print(f"Indexer '{SEARCH_INDEXER_NAME}' loaded data into index '{SEARCH_INDEX_NAME}'.")
